{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxvllbUp74J8foZ6Cn3pIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaitanya-kandagaddala-2006/Chaitanya_projects/blob/main/assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcx2PNjYnD5-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAME:KLV CHAITANYA\n",
        "ROLL NO:24\n",
        "\n",
        "\n",
        "question:-DIffrence between inner and outer product\n",
        "\n",
        "1.The inner product results in a scalar, whereas the outer product results in a matrix.\n",
        "\n",
        "2. The inner product requires the vectors to be of the same length, while the outer product can be performed with vectors of different lengths.\n",
        "\n",
        "3.The inner product is used for similarity measures and projections, while the outer product is used in forming matrices and in higher-order operations.\n",
        "\n",
        "\n",
        "question:STUDY ABOUT NORMS\n",
        "\n",
        "\n",
        "\n",
        "In the context of **Machine Learning (ML)**, the concept of \"norms\" typically refers to mathematical functions used to measure the size or length of vectors (or more generally, the magnitude of elements in a vector space). Norms are critical in various machine learning algorithms, particularly in optimization, regularization, and vector space operations.\n",
        "\n",
        "### Norms in Machine Learning\n",
        "\n",
        "In ML, norms are used in:\n",
        "1. **Measuring Distance**: To evaluate the distance between points in a feature space (e.g., for clustering, classification, etc.).\n",
        "2. **Regularization**: To prevent overfitting by penalizing large model parameters.\n",
        "3. **Optimization**: To find the best parameters in an optimization process (like gradient descent).\n",
        "\n",
        "### Common Types of Norms\n",
        "\n",
        "1. **L2 Norm (Euclidean Norm)**:\n",
        "   - **Definition**: The L2 norm is the most commonly used norm in machine learning. It is the square root of the sum of the squared components of a vector.\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n",
        "     \\]\n",
        "   - **Use in ML**:\n",
        "     - In algorithms like **linear regression** and **support vector machines**, L2 norm is used to measure the error or distance between predictions and actual outcomes.\n",
        "     - It is also used in **regularization** (Ridge regression, for instance) to control the complexity of a model by penalizing large weights.\n",
        "   \n",
        "2. **L1 Norm (Manhattan Norm or Taxicab Norm)**:\n",
        "   - **Definition**: The L1 norm is the sum of the absolute values of the components of the vector.\n",
        "      - **Use in ML**:\n",
        "     - L1 norm is often used in **Lasso regression**, which adds a penalty term based on the sum of absolute values of model coefficients, promoting sparsity (i.e., many coefficients becoming zero).\n",
        "     - It can be useful for feature selection, as it tends to force some feature weights to exactly zero.\n",
        "\n",
        "3. **L∞ Norm (Maximum Norm)**:\n",
        "   - **Definition**: The L∞ norm of a vector is the maximum absolute value among its components.\n",
        "      - **Use in ML**:\n",
        "     - The L∞ norm is less commonly used but can be applied in certain situations where the maximum deviation is more important than the sum or squared sum of deviations.\n",
        "     - It's used in **robust optimization** problems and can be seen in certain types of loss functions or constraints in ML models.\n",
        "\n",
        "4. **Frobenius Norm**:\n",
        "   - **Definition**: The Frobenius norm is a generalization of the L2 norm to matrices. It is the square root of the sum of the squares of all entries in the matrix.\n",
        "     - **Use in ML**:\n",
        "     - It's used for regularizing matrices, often in deep learning, in tasks like **matrix factorization** or for training **autoencoders**.\n",
        "\n",
        "### Applications of Norms in Machine Learning\n",
        "\n",
        "1. **Regularization**:\n",
        "   - Regularization techniques like **Ridge regression** (L2 regularization) and **Lasso regression** (L1 regularization) add a penalty term to the loss function based on the norm of the coefficients. This helps prevent overfitting by limiting the size or number of features used by the model.\n",
        "\n",
        "2. **Distance Metrics**:\n",
        "   - Norms define distance measures between data points. For example, in **K-means clustering**, the L2 norm (Euclidean distance) is typically used to compute the distance between points and the cluster centroids.\n",
        "   - In **K-nearest neighbors (K-NN)** classification, norms (usually L2 or L1) are used to compute the similarity between data points.\n",
        "\n",
        "3. **Gradient Descent and Optimization**:\n",
        "   - During optimization processes like **gradient descent**, norms help in defining the **step size** or update direction for adjusting model parameters.\n",
        "   - The **L2 norm** often appears in the gradient of a loss function, particularly in regularized versions of linear models.\n",
        "\n",
        "4. Deep Learning:\n",
        "   - Norms can be used in **weight initialization** methods and in **batch normalization**.\n",
        "   - **Weight decay** in neural networks is based on the L2 norm of the weight vector, which helps control the growth of model weights and improves generalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7TBd7se6oJ7p"
      }
    }
  ]
}